{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: DATA CLEANING\n",
    "# ============================================================================\n",
    "print(\"STEP 1: LOADING AND CLEANING DATA...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load raw data\n",
    "crop = pd.read_csv('datafile.csv')\n",
    "crop1 = pd.read_csv('datafile (1).csv')\n",
    "crop2 = pd.read_csv('datafile (2).csv')\n",
    "crop3 = pd.read_csv('datafile (3).csv')\n",
    "produce = pd.read_csv('produce.csv')\n",
    "\n",
    "# Clean crop\n",
    "crop.dropna(inplace=True)\n",
    "crop.columns = crop.columns.str.strip()\n",
    "crop.to_csv('cleaned_crop.csv', index=False)\n",
    "\n",
    "# Clean crop1\n",
    "crop1 = crop1[crop1[\"Yield (Quintal/ Hectare) \"] < 68]\n",
    "crop1 = crop1[crop1[\"Cost of Cultivation (`/Hectare) A2+FL\"] < 30000]\n",
    "crop1 = crop1[crop1[\"Cost of Cultivation (`/Hectare) C2\"] < 30000]\n",
    "crop1 = crop1[crop1[\"Cost of Production (`/Quintal) C2\"] < 3000]\n",
    "crop1 = crop1[crop1[\"Yield (Quintal/ Hectare) \"] < 25]\n",
    "crop1.reset_index(inplace=True, drop=True)\n",
    "crop1.columns = crop1.columns.str.strip()\n",
    "crop1.to_csv('cleaned_crop1.csv', index=False)\n",
    "\n",
    "# Clean crop2\n",
    "crop2 = crop2.drop(index=[0, 9, 13, 14, 16, 15, 33, 38, 47])\n",
    "crop2.reset_index(inplace=True, drop=True)\n",
    "crop2.columns = crop2.columns.str.strip()\n",
    "crop2.to_csv('cleaned_crop2.csv', index=False)\n",
    "\n",
    "# Clean crop3\n",
    "crop3 = crop3.drop(columns=[\"Unnamed: 4\"])\n",
    "crop3[\"Variety\"] = crop3[\"Variety\"].str.split().str[0]\n",
    "crop3[\"Season/ duration in days\"] = pd.to_numeric(crop3[\"Season/ duration in days\"], errors='coerce')\n",
    "crop3.dropna(inplace=True)\n",
    "crop3.reset_index(inplace=True, drop=True)\n",
    "crop3.columns = crop3.columns.str.strip()\n",
    "crop3.to_csv('cleaned_crop3.csv', index=False)\n",
    "\n",
    "# Clean produce\n",
    "produce.drop(columns=[\" 3-1993\", \" 3-1994\", \" 3-1995\", \" 3-1996\", \" 3-1997\",\n",
    "                      \" 3-1998\", \" 3-1999\", \" 3-2000\", \" 3-2001\", \" 3-2002\",\n",
    "                      \" 3-2003\", \" 3-2004\"], inplace=True)\n",
    "produce.dropna(inplace=True)\n",
    "produce[\"Unit\"] = (produce[\"Unit\"] == \"Ton mn\")\n",
    "produce = produce[produce[\"Unit\"]]\n",
    "produce[\"Unit\"] = \"Ton mn\"\n",
    "produce.reset_index(drop=True, inplace=True)\n",
    "produce[\"Frequency\"] = produce[\"Frequency\"].str.split(\",\").str[0]\n",
    "produce[\"Particulars\"] = produce[\"Particulars\"].str.split(\" \").str[3:].str.join(\"\")\n",
    "produce = produce.rename(columns={\"Particulars\": \"Crop\"})\n",
    "produce.columns = produce.columns.str.strip()\n",
    "produce.to_csv('cleaned_produce.csv', index=False)\n",
    "\n",
    "print(\"Data cleaning completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: MERGING DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: MERGING DATASETS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reload cleaned data\n",
    "crop = pd.read_csv('cleaned_crop.csv')\n",
    "crop1 = pd.read_csv('cleaned_crop1.csv')\n",
    "crop2 = pd.read_csv('cleaned_crop2.csv')\n",
    "crop3 = pd.read_csv('cleaned_crop3.csv')\n",
    "produce = pd.read_csv('cleaned_produce.csv')\n",
    "\n",
    "# Strip whitespace from columns\n",
    "for df in [crop, crop1, crop2, crop3, produce]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "# Check what columns exist\n",
    "print(\"Available columns in each dataset:\")\n",
    "print(f\"crop: {crop.columns.tolist()}\")\n",
    "print(f\"crop1: {crop1.columns.tolist()}\")\n",
    "print(f\"crop2: {crop2.columns.tolist()}\")\n",
    "print(f\"crop3: {crop3.columns.tolist()}\")\n",
    "print(f\"produce: {produce.columns.tolist()}\")\n",
    "\n",
    "# Start merging - use 'Crop' column if it exists, otherwise concatenate\n",
    "merged_df = produce.copy()\n",
    "\n",
    "if 'Crop' in crop.columns:\n",
    "    merged_df = merged_df.merge(crop, on='Crop', how='left', suffixes=('', '_crop'))\n",
    "    print(f\"\\nMerged with crop. Shape: {merged_df.shape}\")\n",
    "\n",
    "if 'Crop' in crop1.columns:\n",
    "    merged_df = merged_df.merge(crop1, on='Crop', how='left', suffixes=('', '_crop1'))\n",
    "    print(f\"Merged with crop1. Shape: {merged_df.shape}\")\n",
    "\n",
    "if 'Crop' in crop2.columns:\n",
    "    merged_df = merged_df.merge(crop2, on='Crop', how='left', suffixes=('', '_crop2'))\n",
    "    print(f\"Merged with crop2. Shape: {merged_df.shape}\")\n",
    "\n",
    "if 'Crop' in crop3.columns:\n",
    "    merged_df = merged_df.merge(crop3, on='Crop', how='left', suffixes=('', '_crop3'))\n",
    "    print(f\"Merged with crop3. Shape: {merged_df.shape}\")\n",
    "\n",
    "print(f\"\\nFinal merged shape: {merged_df.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: HANDLE MISSING VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: HANDLING MISSING VALUES...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Missing values before cleaning:\")\n",
    "missing_counts = merged_df.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "merged_clean = merged_df.copy()\n",
    "\n",
    "# Drop columns that have more than 90% missing values\n",
    "initial_cols = merged_clean.shape[1]\n",
    "missing_threshold = 0.9 * len(merged_clean)\n",
    "merged_clean.dropna(axis=1, thresh=len(merged_clean) - missing_threshold, inplace=True)\n",
    "dropped_cols = initial_cols - merged_clean.shape[1]\n",
    "print(f\"\\nDropped {dropped_cols} columns with >90% missing values.\")\n",
    "\n",
    "# Fill numeric columns with median\n",
    "numeric_cols = merged_clean.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if merged_clean[col].isnull().any(): # Only fill if there are NaNs\n",
    "        median_val = merged_clean[col].median()\n",
    "        merged_clean[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# Fill categorical columns with mode\n",
    "categorical_cols = merged_clean.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if merged_clean[col].isnull().any(): # Only fill if there are NaNs\n",
    "        mode_val = merged_clean[col].mode()\n",
    "        fill_val = mode_val[0] if len(mode_val) > 0 else 'Unknown'\n",
    "        merged_clean[col].fillna(fill_val, inplace=True)\n",
    "\n",
    "print(f\"Rows after filling NaNs in columns: {len(merged_clean)}\")\n",
    "\n",
    "# Verify no NaN remains at the column level\n",
    "remaining_nan = merged_clean.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values after column-wise cleaning: {remaining_nan}\")\n",
    "\n",
    "# Drop any remaining rows with NaN values (should be minimal after column dropping and filling)\n",
    "if remaining_nan > 0:\n",
    "    print(\"Warning: Some NaN values still remain. Dropping those rows...\")\n",
    "    merged_clean.dropna(inplace=True)\n",
    "    print(f\"Final rows after dropping remaining NaNs: {len(merged_clean)}\")\n",
    "\n",
    "# Ensure there is still enough data after cleaning\n",
    "if len(merged_clean) == 0:\n",
    "    print(\"ERROR: Merged and cleaned DataFrame is empty. Cannot proceed with modeling.\")\n",
    "else:\n",
    "    # Save cleaned merged data\n",
    "    merged_clean.to_csv('merged_crop_data_clean.csv', index=False)\n",
    "    print(\"\\nCleaned merged dataset saved to 'merged_crop_data_clean.csv'\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # STEP 4: PREPARE FOR MODELING\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: PREPARING DATA FOR MODELING...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Get all numeric columns\n",
    "    numeric_cols = merged_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"\\nNumerical columns ({len(numeric_cols)}):\")\n",
    "    for col in numeric_cols:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "    # Identify potential target columns\n",
    "    production_cols = [col for col in numeric_cols if 'production' in col.lower() or '2010-11' in col or '2011' in col]\n",
    "    print(f\"\\nPotential target columns: {production_cols}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # STEP 5: BUILD AND TRAIN MODEL\n",
    "    # ============================================================================\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: TRAINING MODEL...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Select target column - MODIFY THIS based on your actual columns\n",
    "    # Try to find the most recent production column\n",
    "    target_col = None\n",
    "    for col in numeric_cols:\n",
    "        if 'production' in col.lower() and ('2010' in col or '2011' in col):\n",
    "            target_col = col\n",
    "            break\n",
    "\n",
    "    # If no production column found, use the last numeric column\n",
    "    if target_col is None:\n",
    "        target_col = numeric_cols[-1] if numeric_cols else None\n",
    "\n",
    "    if target_col is None:\n",
    "        print(\"ERROR: No suitable target column found!\")\n",
    "        print(\"Available columns:\", merged_clean.columns.tolist())\n",
    "    else:\n",
    "        print(f\"Target column: {target_col}\")\n",
    "\n",
    "        # Define features (all numeric except target)\n",
    "        feature_cols = [col for col in numeric_cols if col != target_col]\n",
    "        print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "        # Create X and y\n",
    "        X = merged_clean[feature_cols]\n",
    "        y = merged_clean[target_col]\n",
    "\n",
    "        # Verify no NaN\n",
    "        print(f\"\\nNaN in X: {X.isnull().sum().sum()}\")\n",
    "        print(f\"NaN in y: {y.isnull().sum()}\")\n",
    "\n",
    "        # Check if we have enough data\n",
    "        if len(X) < 5:\n",
    "            print(\"\\nERROR: Not enough data to train model!\")\n",
    "        else:\n",
    "            # Split data\n",
    "            test_size = min(0.3, 1.0 - (3.0 / len(X)))  # Ensure at least 3 samples in train\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=42\n",
    "            )\n",
    "\n",
    "            print(f\"\\nTrain size: {len(X_train)}\")\n",
    "            print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "            # Train model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            # Calculate metrics\n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "            # ============================================================================\n",
    "            # STEP 6: DISPLAY RESULTS\n",
    "            # ============================================================================\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"MODEL PERFORMANCE:\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            print(f\"\\nTraining Set:\")\n",
    "            print(f\"  R² Score: {train_r2:.4f}\")\n",
    "            print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "            print(f\"  MAE: {train_mae:.4f}\")\n",
    "\n",
    "            print(f\"\\nTest Set:\")\n",
    "            print(f\"  R² Score: {test_r2:.4f}\")\n",
    "            print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "            print(f\"  MAE: {test_mae:.4f}\")\n",
    "\n",
    "            # Feature importance\n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(\"TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "            print(\"=\" * 80)\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_cols,\n",
    "                'Coefficient': model.coef_\n",
    "            }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "            print(feature_importance.head())\n",
    "\n",
    "            # Save predictions\n",
    "            results_df = pd.DataFrame({\n",
    "                'Actual': y_test.values,\n",
    "                'Predicted': y_test_pred,\n",
    "                'Error': y_test.values - y_test_pred\n",
    "            })\n",
    "            results_df.to_csv('prediction_results.csv', index=False)\n",
    "            print(\"\\nPredictions saved to 'prediction_results.csv'\")\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"COMPLETED SUCCESSFULLY!\")\n",
    "            print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a86856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample input for prediction.\n",
    "# These values should correspond to the feature columns used in the model.\n",
    "# You can modify these values to get different predictions.\n",
    "sample_input = pd.DataFrame({\n",
    "    '3-2005': [100.0],\n",
    "    '3-2006': [110.0],\n",
    "    '3-2007': [120.0],\n",
    "    '3-2008': [130.0],\n",
    "    '3-2009': [140.0],\n",
    "    '3-2010': [150.0],\n",
    "    '3-2011': [160.0],\n",
    "    '3-2012': [170.0],\n",
    "    '3-2013': [180.0],\n",
    "    '3-2014': [190.0],\n",
    "    'Production 2006-07': [100.0],\n",
    "    'Production 2007-08': [110.0],\n",
    "    'Production 2008-09': [120.0],\n",
    "    'Production 2009-10': [130.0],\n",
    "    'Area 2006-07': [50.0],\n",
    "    'Area 2007-08': [55.0],\n",
    "    'Area 2008-09': [60.0],\n",
    "    'Area 2009-10': [65.0],\n",
    "    'Area 2010-11': [70.0],\n",
    "    'Yield 2006-07': [10.0],\n",
    "    'Yield 2007-08': [7.0],\n",
    "    'Yield 2008-09': [12.0],\n",
    "    'Yield 2009-10': [13.0],\n",
    "    'Yield 2010-11': [14.0]\n",
    "})\n",
    "\n",
    "# Ensure the columns are in the same order as the training features\n",
    "sample_input = sample_input[feature_cols]\n",
    "\n",
    "# Make a prediction\n",
    "predicted_value = model.predict(sample_input)\n",
    "\n",
    "print(f\"Predicted Production for 2010-11: {predicted_value[0]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
